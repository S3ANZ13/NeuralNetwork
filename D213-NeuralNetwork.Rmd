---
always_allow_html: true
title: "D213 - Advanced Data Analytics"
author: "Sean P. Murphy"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
  always_allow_html: true
  html_notebook:
    toc: true
    toc_depth: 3
subtitle: Sentiment Analysis using Neural Networks
---

\newpage

# Introduction

  Prior to beginning the assigned sentiment analysis, we will provide some sense of the how this assignment fits into the overall context of our coursework so far and also prepare the environment for this project.

## Assignment Background

  In our previous coursework, we were tasked by a popular hospital chain with analyzing several datasets, each with a slightly different focus.  Initially, we identified criteria to quantify risk levels for patient readmission within one month of their initial discharge. When provided with patient prescription histories, we uncovered patterns that revealed the potential for medication interaction risks.  Using patient survey data, we uncovered the existence of two specific groups of patients with distinct and diverging priorities so that the hospital can tailor individual marketing and patient care strategies more appropriately. Lastly, we provided daily revenue benchmarks against which policymakers can compare hospital revenue after implementing changes in policy or strategy to assess their relative success.
  
  For this assignment, we will 

## Initial Preparation

```{r Desired Libraries}
# Visualization Formatting
library(gridExtra)
library(viridisLite)

# Visualization Font Import
library(sysfonts)
library(showtextdb)
library(showtext)
font_add("LM Roman 10", "./lmroman10-regular.otf")
```


  We will be loading a number of required libraries to perform some simple data manipulation (*dplyr* and *purr*), create visualizations (*ggplot2*), and to perform the bulk of the sentiment analysis analysis (*keras*).


```{r Required Libraries, message=FALSE, warning=FALSE}
# Data Manipulation and Visualization
library(tidyverse)
library(magrittr)
library(tidytext)

# ML Libraries
library(keras)
library(mlbench)
library(neuralnet)
library(SnowballC)
```


\newpage
***
# Part I:  Research Scope
***

## A1: Research Question

> Describe the purpose of this data analysis by doing the following:: Summarize one research question that you will answer using neural network models and NLP techniques. Be sure the research question is relevant to a real-world organizational situation and sentiment analysis captured in your chosen dataset.

  

## A2: Research Objective

> Describe the purpose of this data analysis by doing the following:: Define the objectives or goals of the data analysis. Be sure the objectives or goals are reasonable within the scope of the research question and are represented in the available data.

## A3: Research Mechanism

> Describe the purpose of this data analysis by doing the following:: Identify a type of neural network capable of performing a text classification task that can be trained to produce useful predictions on text sequences on the selected data set.


\newpage
***
# Part II:  Data Preparation
***

  We will first import our selected data sets.  This labeled data is similarly formatted and comes from three different sources: Amazon product reviews, IMDb movie reviews, and Yelp business reviews.  Each file contains the original text of the respective reviews and their human-assessed sentiment: positive or negative.
  
### Data Import

  We will be using the *readr* function imported as part of the *tidyverse*. There are no headers, so we will ensure that each row is condisereded as an observation and add our own labels at a later step. Each dataset will be imported individually and combined into a single dataframe afterward.
  
```{r Import Data, message=FALSE}
df.imdb   <- readr::read_csv('./imdb_labelled.csv', col_names = FALSE)
df.yelp   <- readr::read_csv('./yelp_labelled.csv', col_names = FALSE)
df.amazon <- readr::read_csv('./amazon_cells_labelled.csv', col_names = FALSE)
```

### Data Concatenation

  We will now amalgamate each of these data frames together using *rbind* and add appropriate column labels. We will convert the sentiment to *factor* data and print a *summary* of the resulting dataframe to examine its properties.

```{r Combine Data}
df <- rbind(df.amazon, df.yelp, df.imdb)
colnames(df) <- c('Review', 'Sentiment')
df$Sentiment <- as.factor(df$Sentiment)
summary(df)
```

## B1: Exploratory Data Analysis

  As we saw above in the dataframe summary, there are an equal number of positive and negative observations across the product, movie, and business reviews.  This will help ensure that we do not produce skewed results from a selction bias.

```{r Sentiment Distribution, echo=FALSE, fig.height=2.5, fig.showtext=TRUE}
df.amazon$X2 <- as.factor(df.amazon$X2)
colnames(df.amazon) <- c('Review', 'Sentiment')
amazon <- ggplot(df.amazon, aes( x = Sentiment, y = after_stat(count) ) ) + 
            geom_bar(aes(fill = Sentiment), stat = "count", position = "dodge" ) +
            labs(title = "Amazon",
            y="Count", x=" ") +
            theme(text = element_text(family = "LM Roman 10")) +
            scale_fill_viridis_d(labels=c("Negative", "Positive")) +
            theme(legend.position = "none")

df.yelp$X2 <- as.factor(df.yelp$X2)
colnames(df.yelp) <- c('Review', 'Sentiment')
yelp <- ggplot(df.yelp, aes( x = Sentiment, y = after_stat(count) ) ) + 
            geom_bar(aes(fill = Sentiment), stat = "count", position = "dodge" ) +
            labs(title = "Yelp",
            y=NULL, x="Sentiment") +
            theme(text = element_text(family = "LM Roman 10")) +
            scale_fill_viridis_d(labels=c("Negative", "Positive")) +
            theme(legend.position = "none")

df.imdb$X2 <- as.factor(df.imdb$X2)
colnames(df.imdb) <- c('Review', 'Sentiment')
imdb <- ggplot(df.imdb, aes( x = Sentiment, y = after_stat(count) ) ) + 
            geom_bar(aes(fill = Sentiment), stat = "count", position = "dodge" ) +
            labs(title = "IMDb",
            y=NULL, x=" ") +
            theme(text = element_text(family = "LM Roman 10")) +
            scale_fill_viridis_d(labels=c("Negative", "Positive")) +
            theme(legend.position = "none")

grid.arrange(amazon, yelp, imdb, ncol=3)
rm(amazon, yelp, imdb)
```

  Now we will give our data a cursory examination prior to designing the neural network that we will use to classify the sentiment of each review using natural language processing.

#### Amazon Data:

  For context and our general awareness, we will examine the first five rows of the Amazon data set.
  
```{r Amazon Data Head}
head(df[1:5,])
```


#### Yelp Data:

  Similarly, we will examine the first five rows of observations from the Yelp dataset.

```{r Yelp Data Head}
head(df[1001:1005,])
```


#### IMDb Data:

  Finally, we will examine the first five rows of movie review data from the IMDb dataset.

```{r IMDb Data Head}
head(df[2001:2005,])
```


\newpage

### Lowercase Characters

  In order to standardize the characters and ensure that like words are counted together, we will force all letters to lowercase.  This will also decrease the vector space required to store our data which will reduce the overall computational overhead.
  
#### Before:

  As the data sets were provided, there is no standard format for user-generated input.  Most people use sentence case standards, this is neither consistent nor useful for our purposes.
  
```{r Lowercase Letters Before}
head(df, n=3)
```

#### Conversion:

  We will use the *tolower* function as part of the base R language to convert all character data to lowercase letters.
  
```{r Lowercase Letters Conversion}
df$Review <- tolower(df$Review)
```


#### After:

  Now, we see below that all letters are represented using lower case characters.

```{r Lowercase Letters After}
head(df, n=3)
```


### Unusual Characters 

  In order to reduce the overall character set, we will want to address sentiment-agnostic characters like punctuation, emojis, non-English characters, etc.
  
#### Before:

  We see below that the usage of special characters varies widely between proper punction to emjois.

```{r Unusal Letters Before}
head(df[grep("[^a-zA-Z0-9]", df$Review), ], n=10)
```

#### Conversion:

  We will remove all unwanted unusual characters to reduce the extraneous portions of the vector space by replacing thost characters with spaces that will not be tokenized later on. 

```{r}
df$Review <- gsub("[^a-zA-Z0-9]", " ", df$Review)
```


#### After:

  Now, we see below that all the special characters have been removed.
  
```{r}
head(df[grep("[^a-zA-Z0-9]", df$Review), ], n=10)
```

### Vocabulary Size

  A very common problem with sentiment analysis using a body of text is that many of the words that we use with significant frequency do not convey any information about the feelings of the speaker. 
  
```{r}
df %>%
  unnest_tokens(word, Review) %>%
  count(word) %>%
  nrow()
```


 
### Word Embedding Length



### Maximum Sequence Length


```{r Review Length Histogram, echo=FALSE, fig.showtext=TRUE}
a <- data.frame(nchar(df$Review))
colnames(a) <- c('Length')

ggplot(a, aes(x=Length)) + 
  geom_histogram(binwidth=10, fill="#440154", color="#481567", aes(y=..density..)) +
  geom_density(color="#21918c") +
  labs(title = "Review Length Histogram",
  y="Density", x="Number of Characters") +
  theme(text = element_text(family = "LM Roman 10"))

rm(a)
```

  We note that cutting off our review character length at $< 200$ leaves `r (sum(((nchar(df$Review)) <200) == TRUE)/1000)*100`% of the reviews untouched.  This also brings our maximum length down to less than half of the longest review in the original dataset.


Split reviews into tokens separated by whitespace
5. Removed non-alphabetic and non-numeric words
6. Removed stop words from tokens
7. Combine remaining tokens back into sentences


## B2: Tokenization

> Summarize the data cleaning process by doing the following: Describe the goals of the tokenization process, including any code generated and packages that are used to normalize text during the tokenization process.

tidytext

  We will be reducing the text array in the review content column to individual words.
  
```{r Tokenize Text}
df.clean <- df %>%
  unnest_tokens(word, Review)
```

 With the individual words separated, we will examine the most frequently used words in the text of the IMDb movie reviews.  We notice that the top twenty words in the body IMDb review texts do not convey the speakers feelings about the movie.
 

```{r Distinct Word Count}
df.clean %>%
  count(word, sort=TRUE) %>%
  head(20)
```


 We will remove these common stop words from the review content before proceeding.  This will help enforce the principle of least effort and remove words that do not help identify the sentiment of a body of text.
  

```{r Remove Stop Words}
df.clean <- anti_join((unnest_tokens(df.clean, word, word)), stop_words)

df.clean %>% count(word, sort = TRUE)
```

  Based on our cursory observation above, we note that some of the more common words can also be removed due to their sentiment neutrality.
  
```{r Custom Lexicon}
custom <- add_row(stop_words, word="movie", lexicon="custom")
custom <- add_row(custom, word="movies", lexicon="custom")
custom <- add_row(custom, word="movie's", lexicon="custom")

df.clean <- anti_join(df.clean, custom)

df.clean %>% count(word, sort = TRUE)
```

  Lastly, for this section, we will apply a stemming function.  This will render some words slightly less human-readable but it will consolidate words with similar meanings but varying forms such that their overall impact isn't artificially weakened.

```{r Word Stemming}
df.clean <- mutate(df.clean, word = wordStem(word))

df.clean %>% count(word, sort = TRUE)
```


## B3: Padding

> Summarize the data cleaning process by doing the following: Explain the padding process used to standardize the length of sequences, including the following in your explanation:

  In order to make the body of the review text more compatible with a neural network analysis, we could one-hot-encode the arrays based on the $n$ most common words, creating an $n$-dimensional vector.  However, this would be considerably memory intensive; hence, we will choose to pad the arrays instead. 

  Rather than one-hot encoding our text to $n$-dimensional vectors, we will coerce each of the arrays into having the same length regardless of the number of characters in the review itself.


  
•   if the padding occurs before or after the text sequence

•   a screenshot of a single padded sequence


## B4: Sentiment Categories

  Our selected dataset has two categories: positive and negative.  According to the data dictionary that accompanied the raw data, the sentiment values are coded numerically such that 1 indicates a positive sentiment and 0 indicates a negative sentiment (Kotzias et al, 2015).  The sentiment categories are evenly divided to reduce the potential for sample bias.

```{r Clean Data Summary}
summary(df.clean$Sentiment)
```


## B5: Train/Test Split

  We will adhere to a fairly common 80/20 train/test split for the purposes of this project.

```{r Test-Train Split}
set.seed(213)
split.index <- sample.int(nrow(df.clean), size = nrow(df)*0.8)
df.train <- df.clean[split.index,]
df.test <- df.clean[-split.index,]
```

```{r include=FALSE}
rm(split.index)
```


```{r Test-Train Split Distribution, echo=FALSE, fig.showtext=TRUE, fig.height=3}
test <- ggplot(df.train, aes( x = Sentiment, y = after_stat(count) ) ) + 
            geom_bar(aes(fill = Sentiment), stat = "count", position = "dodge" ) +
            labs(title = "Training Data",
            y="Count", x=" ") +
            theme(text = element_text(family = "LM Roman 10")) +
            scale_fill_viridis_d(labels=c("Negative", "Positive")) +
            theme(legend.position = "none")

train <- ggplot(df.test, aes( x = Sentiment, y = after_stat(count) ) ) + 
            geom_bar(aes(fill = Sentiment), stat = "count", position = "dodge" ) +
            labs(title = "Testing Data",
            y="Count", x=" ") +
            theme(text = element_text(family = "LM Roman 10")) +
            scale_fill_viridis_d(labels=c("Negative", "Positive")) +
            theme(legend.position = "none")

grid.arrange(test, train, ncol=2)
rm(test, train)
```




## B6: Data Export

  We will save the IMDb review data as a training set and a testing set as divided above using a pseudo-random sample function.

```{r Data Export}
write.csv(df.train, './nn_testing.csv', row.names = FALSE)
write.csv(df.test, './nn_training.csv', row.names = FALSE)
```




\newpage
***
# Part III:  Network Architecture
***




## C1: Model Summary

> Describe the type of network used by doing the following: Provide the output of the model summary of the function from TensorFlow.

## C2: Layers and Parameters

> Describe the type of network used by doing the following: Discuss the number of layers, the type of layers, and total number of parameters.

### Text Vectorization Layer

  We define our text vectorization later here.  It will take our string input and convert that data to a tensor.
  
```{r Text Vectorization Layer Creation, message=FALSE, warning=FALSE}
num_words <- 10000
max_length <- 10
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)
```

```{r Text Vectorization Layer Adaptation}
text_vectorization %>% 
  adapt(df.clean$word)
head(get_vocabulary(text_vectorization))
```


```{r Text Vectorization Matrix}
text_vectorization(matrix(df.clean$word[1], ncol = 1))
```



## C3: Hyperparameters

> Describe the type of network used by doing the following: Justify the choice of hyperparameters, including the following elements:

```{r}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(input, output)
```


### Loss Function

  (reword) Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we’ll use the binary_crossentropy loss function.  We could, for instance, choose mean_squared_error. But, generally, binary_crossentropy is better for dealing with probabilities — it measures the “distance” between probability distributions, or in our case, between the ground-truth distribution and the predictions.
  
```{r Configure Loss Function, eval=FALSE}
model %>% compile(loss = 'binary_crossentropy')
```


### Optimizer

```{r Configure Optimizer, eval=FALSE}
model %>% compile(optimizer = 'adam')
```


### Activation Functions

  ReLU

  (reword) The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.


### Number of Nodes per Layer


### Stopping Criteria


### Evaluation Metric

  Accuracy

```{r}
model %>% compile(
  optimizer = 'adam',
  loss = 'binary_crossentropy',
  metrics = list('accuracy')
)
```



\newpage
***
# Part IV:  Model Evaluation
***

  With the network layers and parameters discussed above, we will not train and evaluate the model.
  
```{r Train the Model}
history <- model %>% fit(
            df.train$word,
            as.numeric(df.train$Sentiment == 1),
            epochs = 20,
            batch_size = 100,
            validation_split = 0.1,
            verbose=2)
```

  

## D1: Final Epoch

> Evaluate the model training process and its relevant outcomes by doing the following: Discuss the impact of using stopping criteria instead of defining the number of epochs, including a screenshot showing the final training epoch.


## D2: Loss and Accuracy

> Evaluate the model training process and its relevant outcomes by doing the following: Provide visualizations of the model’s training process, including a line graph of the loss and chosen evaluation metric.

```{r Loss and Accuracy}
results <- model %>% evaluate(df.test$word, as.numeric(df.test$Sentiment == 1), verbose = 0)
results
```


## D3: Model Fitness

> Evaluate the model training process and its relevant outcomes by doing the following: Assess the fitness of the model and any measures taken to address overfitting.


## D4: Predictive Accuracy

> Evaluate the model training process and its relevant outcomes by doing the following: Discuss the predictive accuracy of the trained network.

```{r Graph of Accuracy}
plot(history)
```



\newpage
***
# Part V:  Summary and Recommendations
***

## E: Code Copy

> Provide the code used to save the trained network within the neural network.

```{r Code Copy}
df$Review %>% 
  strsplit(" ") %>% 
  sapply(length) %>% 
  summary()

set.seed(213)
split.index <- sample.int(nrow(df), size = nrow(df)*0.8)
training <- df[split.index,]
testing <- df[-split.index,]

num_words <- 10000
max_length <- 10
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)


text_vectorization %>% 
  adapt(df$Review)

head(get_vocabulary(text_vectorization))


text_vectorization(matrix(df$Review[1], ncol = 1))


input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(input, output)


model %>% compile(
  optimizer = 'adam',
  loss = 'binary_crossentropy',
  metrics = list('accuracy')
)

history <- model %>% fit(
            training$Review,
            as.numeric(training$Sentiment == 1),
            epochs = 15,
            batch_size = 100,
            validation_split = 0.1,
            verbose=2)

results <- model %>% evaluate(testing$Review, as.numeric(testing$Sentiment == 1), verbose = 0)
results
plot(history)
```


## F: Code Functionality

> Discuss the functionality of your neural network, including the impact of the network architecture.

## G: Recommendations

  Despite the "black box" nature of Sentiment Analysis as a subset of Natural Language Processing methods using Deep Neural Networks, we have successfully demonstrated the requsted proof of concept as directed by hospital administrators.


\newpage
***
# Part VI: Reporting
***

## H: IDE Selection

  The entirety of this submission was authored within an R Notebook using RStudio Server version 2022.02.2, build number 485 (RStudio Team, 2022).  This instance was installed on a virtual instantiation of Ubuntu Server 20.04.4 LTS.
  
  As we have done with all other submissions in this program, we will include a knitted pdf of this executed notebook.


## I-J: Academic Integrity

  All sources for research and code usage have been cited in-text where appropriate, and a complete listing of these references is provided below.


## K:  Communication Standards
  
  Content has been edited for professionalism and appropriate communication standards. The formats of this submission adhere to rubric requirements.
  

\newpage

# References

Kotzias, D., Denil, M., Freitas, N. & Smyth, P. 2015. From Group to Individual Labels Using Deep Features. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '15). Association for Computing Machinery, New York, NY, USA, 597–606. 
https://doi.org/10.1145/2783258.2783380

Maas, A., Daly, R., Pham, P.,  Huang, D., Ng, A. & Potts, C. (2011). Learning Word Vectors for Sentiment Analysis. 142-150. https://aclanthology.org/P11-1015

RStudio Team (2022). RStudio: Integrated Development Environment for R. RStudio, PBC, Boston, MA. http://www.rstudio.com/.


https://towardsdatascience.com/understanding-relu-the-most-popular-activation-function-in-5-minutes-459e3a2124f
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6